{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "args {'model': 'MoNet', 'epochs': 60, 'batch_size': 1024, 'dropout': 0.001, 'seed': '1234', 'gpu': 0, 'save': 'save/model.pt', 'cuda': True, 'nn': 16, 'lr': 0.001, 'sigma': None, 'embedding': 40, 'cv': 0, 'numlayers': 2}\n",
      "==> Preparing data..\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.backends.cudnn as cudnn\n",
    "from Optim import Optim\n",
    "import torch.optim as optim\n",
    "import argparse\n",
    "import random\n",
    "import numpy as np\n",
    "import timeit\n",
    "from models import GCN, ChebyNet, MoNet, DSGC\n",
    "from utils import computeLaplacian\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from CustomCVs import StratifiedKFoldMixedSizes, StratifiedKFoldByGroups\n",
    "from data_handling_Monet_DTI import create_data_set\n",
    "from sklearn.model_selection import GridSearchCV, GroupKFold, LeaveOneGroupOut\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score,roc_auc_score,confusion_matrix\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score,cross_val_predict,StratifiedKFold\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# parser 객체 생성\n",
    "parser = argparse.ArgumentParser(description='PyTorch Time series forecasting')\n",
    "# argument 추가\n",
    "parser.add_argument('--model', type=str, default='DPIEnn' ,help='Model Name')\n",
    "parser.add_argument('--epochs', type=int, default=60,help='upper epoch limit')\n",
    "parser.add_argument('--batch_size', type=int, default=128, metavar='N',help='batch size')\n",
    "parser.add_argument('--dropout', type=float, default=0.5, help='dropout applied to layers (0 = no dropout)')\n",
    "parser.add_argument('--seed', type=int, default=1234,help='random seed')\n",
    "parser.add_argument('--gpu', type=int, default=0)\n",
    "parser.add_argument('--save', type=str,  default='save/model.pt',help='path to save the final model')\n",
    "parser.add_argument('--cuda', type=str, default=True, help='use gpu or not')\n",
    "parser.add_argument('--nn', type=int, default=16, help='number of the nearest neighbors')\n",
    "parser.add_argument('--lr', type=float, default=0.001, help='learning rate')\n",
    "parser.add_argument('--sigma', type=float, default=None, help='sigma coefficient.')\n",
    "parser.add_argument('--embedding', type=int, default=40, help='embedding_num')\n",
    "parser.add_argument('--cv', type=int, default=0, help='cv type: 0, 1, 2')\n",
    "parser.add_argument('--numlayers', type=int, default=2, help='# of layers')\n",
    "\n",
    "#parser.add_argument('--nn', type=str, default='8,8,8,8,3', help='number of the nearest neighbors')\n",
    "#parser.add_argument('--l', type=float, default=0.25, help='laplacian constant for testing')\n",
    "\n",
    "# method로 명령창에서 주어진 인자를 args 이름으로 파싱\n",
    "#jupyter notebook에서는 이 부분 에러 발생\n",
    "# : argparse 라이브러리를 사용하기 원한다면 터미널이나 다른 프레임워크에서 실행\n",
    "#args = parser.parse_args()\n",
    "\n",
    "import easydict\n",
    "\n",
    "# 전체 최고 : 50 1024 : 54 / 54\n",
    "# test 최고 : 60 1024\n",
    " \n",
    "args = easydict.EasyDict({\n",
    "    \"model\": \"MoNet\",\n",
    "    \"epochs\": 60,\n",
    "    \"batch_size\": 1024,\n",
    "    \"dropout\": 0.001,\n",
    "    \"seed\": \"1234\",\n",
    "    \"gpu\": 0,\n",
    "    \"save\": 'save/model.pt',\n",
    "    \"cuda\": True,\n",
    "    \"nn\": 16,\n",
    "    \"lr\": 0.001,\n",
    "    \"sigma\": None,\n",
    "    \"embedding\": 40,\n",
    "    \"cv\": 0,\n",
    "    \"numlayers\": 2,\n",
    "})\n",
    "\n",
    "print('args', args)\n",
    "\n",
    "# Data\n",
    "print('==> Preparing data..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "#set a device\n",
    "# gpu/cpu를 통해서 학습 및 test 실행\n",
    "device = torch.device('cuda:'+str(args.gpu) if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "cudnn.benchmark = True\n",
    "bShowParameter = False\n",
    "\n",
    "if bShowParameter:\n",
    "    for name, param in model.named_parameters():\n",
    "        if param.requires_grad:      # requires_grad == True : tensor의 모든 연산에 대하여 추적& 기록\n",
    "            print(name, param.data.size())\n",
    "\n",
    "# #print # of parameters\n",
    "# nParams = sum([p.nelement() for p in model.parameters()])\n",
    "# print('* number of parameters: %d' % nParams)\n",
    "\n",
    "def miniBatchDat(X,y,batchSize = 128,del_idx=None):\n",
    "    n = len(X)\n",
    "    # X = normalize(X, axis=0, norm='l1')\n",
    "\n",
    "    # n개의 data를 batchSize별로 나누고 그 batch들의 시작점마다\n",
    "    for begin in range(0, n, batchSize):\n",
    "        # y = torch.Tensor(y[begin:begin + batchSize]).type(torch.LongTensor)\n",
    "        #if not del_idx:\n",
    "            # X, y data에서 나눠진 batch들의 Tensor를 2차원으로 \n",
    "        #    yield torch.unsqueeze(torch.Tensor(X[begin:begin + batchSize, :]),2), torch.Tensor(y[begin:begin + batchSize]).long()\n",
    "        #else:\n",
    "            X1 = np.delete(X[begin:begin + batchSize, :],del_idx,1)\n",
    "            X2 = X[begin:begin + batchSize, del_idx]\n",
    "            yield torch.unsqueeze(torch.Tensor(X1),2), torch.Tensor(X2), torch.Tensor(y[begin:begin + batchSize]).long()\n",
    "\n",
    "# 가중치 초기화\n",
    "def weights_init(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        m.reset_parameters()\n",
    "\n",
    "#training\n",
    "def train(model,epoch,X,y):\n",
    "    # training 시작 시간\n",
    "    time_st = timeit.default_timer()\n",
    "    # 학습\n",
    "    model.train()\n",
    "    \n",
    "    train_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    batch_idx = 0\n",
    "    \n",
    "    #for i1, i2 in miniBatchDat(X,y,args.batch_size,del_idx):\n",
    "    #    print(i1, i2)\n",
    "    # batch로 나눠서 batch 순서대로 학습 진행 + loss 계산 + optimize\n",
    "    for in1,in2, targets in miniBatchDat(X,y,args.batch_size,del_idx):\n",
    "        in1,in2, targets = in1.to(device),in2.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        \"\"\"\n",
    "        print('in1: ' + str(in1))\n",
    "        print('in1 type: ' + str(type(in1)))\n",
    "        print('in1 size: ' + str(in1.size()))\n",
    "        \"\"\"\n",
    "\n",
    "        #print('in1 size: ' + str(in1.size())): in1 size: torch.Size([64, 251, 1])\n",
    "        #print('targets size: ' + str(targets.size())): targets size: torch.Size([64])\n",
    "        \n",
    "        #print(len(in1), len(in1[0]), len(in1[0][0]))\n",
    "        if args.model == 'MoNet':\n",
    "            outputs = model(in1)\n",
    "        else: \n",
    "            outputs = model(in1,in2)\n",
    "        \n",
    "        #print(outputs, targets)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        batch_idx += 1\n",
    "\n",
    "    # for name, param in model.named_parameters():\n",
    "    #     if param.requires_grad:\n",
    "    #         print(name, param.data, param.grad)\n",
    "    train_time = timeit.default_timer() - time_st\n",
    "    print('[%3d/%4d] '%(epoch, args.epochs),'Training Time: %2f'% train_time, 'Loss: %.3f | Acc: %.3f%% (%d/%d)'%(train_loss/(batch_idx+1),100.*correct/total, correct, total), end =\" \")\n",
    "\n",
    "\n",
    "#testing\n",
    "def test(purpose, model, X, y):\n",
    "    #print(\"test 중\")\n",
    "    time_st = timeit.default_timer()\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        batch_idx = 0\n",
    "        for in1, in2, targets in miniBatchDat(X, y, args.batch_size, del_idx):\n",
    "            in1, in2, targets = in1.to(device), in2.to(device), targets.to(device)\n",
    "            if args.model == 'MoNet': \n",
    "                outputs = model(in1)\n",
    "                #print(in1, outputs)\n",
    "            else: \n",
    "                outputs = model(in1,in2)\n",
    "                \n",
    "            for i in range(list(targets.size())[0]):\n",
    "                if targets[i] < 0: targets[i] = 0\n",
    "                    \n",
    "            loss = criterion(outputs, targets)\n",
    "            test_loss += loss.item()\n",
    "            _, predicted = outputs.max(1)\n",
    "            #print(\"test 중:\", predicted)\n",
    "            total += targets.size(0)\n",
    "            #print(predicted)\n",
    "            #print(targets)\n",
    "            #print(predicted.eq(targets))\n",
    "            #print(predicted.eq(targets).sum())\n",
    "            correct += predicted.eq(targets).sum().item()\n",
    "            batch_idx += 1\n",
    "    test_time = timeit.default_timer() - time_st\n",
    "    test_acc = 100.*correct/total\n",
    "\n",
    "    print('Testing Time: %2f'% test_time, 'Acc: %.3f%% (%d/%d)'%(test_acc, correct, total))\n",
    "    if(purpose == 'acc'):\n",
    "        return test_acc\n",
    "    else:\n",
    "        return predicted\n",
    "\n",
    "best_acc = 0.0\n",
    "# best_epoch = 0\n",
    "cv_type = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ENIGMA dataset: Complete = True, Min_Threshold = 0.9, Covariates = ['site'], Min_Counts_per_Site = auto, y_label \n",
      "\n",
      "X:  938\n",
      "Finished loading data set: 938 samples, 248 FS features, 1 covariates \n",
      " \n",
      " \n",
      "test set 설정\n",
      "Loading ENIGMA dataset: Complete = True, Min_Threshold = 0.9, Covariates = ['site'], Min_Counts_per_Site = auto, y_label \n",
      "\n",
      "X:  314\n",
      "Finished loading data set: 308 samples, 248 FS features, 1 covariates \n",
      " \n",
      " \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rlatn\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py:966: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "# Here we will import our data from the csv: we do NOT drop out participants with incomplete features (complete=False), but they should have at least 90% feature completeness (completeness_threshold=0.9), we add Age and Sex and SiteID (age_group_tesla_site) as extra columns (covariates), and we automaticly exclude sites with too little participants (min_counts_per_site). I added documentation to this function, check it for further details.\n",
    "\n",
    "# ENIGMA_OCD_26-01-2019.csv 파일 upload하여 data 이용\n",
    "X,fs_labels,cov,y, groups = create_data_set(complete=True, completeness_threshold=0.9,\n",
    "                        covariates=['site'], min_counts_per_site='auto')\n",
    "\n",
    "#print('x:{},fs_labels:{},cov:{},y:{},groups:{}'.format(X,fs_labels,cov,y,groups))\n",
    "# X : 각 환자별로 정보 저장\n",
    "# y : 강박증 여부 (0-강박증x / 1-강박증ㅇ)\n",
    "# fs_labels : 뇌 구조와 관련된 열의 이름 값\n",
    "# groups : site 열의 모든 행 값\n",
    "\n",
    "#print(relevant_features)\n",
    "#print(\"X: \", X)\n",
    "#print(\"y: \", y)\n",
    "#print(\"cov: \", cov)\n",
    "#print(\"fs_labels: \", fs_labels)\n",
    "#print(\"groups: \", groups)\n",
    "\n",
    "\"\"\"\n",
    "df = pd.read_csv('./dat/dti.merge13.csv')\n",
    "dfdf = pd.DataFrame()\n",
    "covariates = ['Age', 'Sex', 'site']\n",
    "\n",
    "for i in range(len(df.columns)):\n",
    "    if(i>=20):\n",
    "        dfdf[df.columns[i]] = df[df.columns[i]].reset_index(drop=True)\n",
    "        \n",
    "X = dfdf.dropna(axis=0)  # 뇌 구조물들 사이의 tract 값들 저장\n",
    "y = df['Dx']\n",
    "fs_labels = df.columns[20:]\n",
    "#cov = df.iloc[df.columns['Age', 'Sex', 'site']]\n",
    "#groups\n",
    "\"\"\"\n",
    "# dai와 train set 동일시\n",
    "X_test,fs_labels_test,cov_test,y_test, groups_testing = create_data_set(TesTrain=\"Test\", complete=True, completeness_threshold=0.9,\n",
    "                        covariates=['site'], min_counts_per_site='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X shape:(938, 248)\n"
     ]
    }
   ],
   "source": [
    "print('X shape:{}'.format(X.shape))\n",
    "#print(relevant_features)\n",
    "#print(cov)\n",
    "\n",
    "# fs_labels : csv에서 사용된 feature들 저장된 배열\n",
    "#print(fs_labels)\n",
    "\n",
    "feat_surf = []\n",
    "feat_thick = []\n",
    "\n",
    "feat_idx = []\n",
    "x_del_idx = []\n",
    "\n",
    "del_idx = []\n",
    "\n",
    "def computeDist(df,feat1,feat2,min,max,group):\n",
    "    # dtype='f8' : a, b 배열을 float64로 맞춰주기 위한 코드\n",
    "    a = np.array(df.loc[feat1].tolist()[:-1], dtype='f8')  # .reshape(1, -1)\n",
    "    b = np.array(df.loc[feat2].tolist()[:-1], dtype='f8')  # .reshape(1, -1)\n",
    "    \n",
    "    for idx in range(3):\n",
    "        a[idx] = (a[idx] - min[group][idx])/(max[group][idx] - min[group][idx])\n",
    "        b[idx] = (b[idx] - min[group][idx])/(max[group][idx] - min[group][idx])\n",
    "\n",
    "    return np.linalg.norm(a - b)\n",
    "\n",
    "# 각 label별 min, max 계산 + 저장\n",
    "def computeMinMax(row,min,max,group):\n",
    "    for idx in range(3):\n",
    "        if min[group][idx] > float(row[idx]):\n",
    "            min[group][idx] = float(row[idx])\n",
    "        if max[group][idx] < float(row[idx]):\n",
    "            max[group][idx] = float(row[idx])\n",
    "    return min,max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dist: [[0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
      " ...\n",
      " [0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5 ... 0.5 0.5 0.5]\n",
      " [0.5 0.5 0.5 ... 0.5 0.5 0.5]]\n",
      "938 248\n"
     ]
    }
   ],
   "source": [
    "if args.model == 'GCN' or args.model == 'DSGC' or args.model == 'ChebyNet':\n",
    "\n",
    "    df = pd.read_csv('./dat/dictionary.csv', index_col=0)\n",
    "    #print(df.index)\n",
    "\n",
    "    dist = np.zeros((fs_labels.shape[0], fs_labels.shape[0]))\n",
    "    max = [[-9999,-9999,-9999],[-9999,-9999,-9999],[-9999,-9999,-9999]]\n",
    "    min = [[9999,9999,9999],[9999,9999,9999],[9999,9999,9999]]\n",
    "\n",
    "    del_idx = []\n",
    "    index = df.index\n",
    "    #print(df.index.values)\n",
    "\n",
    "    for feat1 in index:\n",
    "        # if label_group[feat1] == 1:\n",
    "        row = df.loc[feat1].tolist()[:-1]\n",
    "        if(not (df.loc[feat1].tolist()[3]=='skip' or df.loc[feat1].tolist()[3]==' skip')):\n",
    "            #print(row)\n",
    "            min,max = computeMinMax(row,min,max,label_group[feat1] - 1)\n",
    "\n",
    "\n",
    "    #remove LSurfArea,RSurfArea,LThickness,RThickness\n",
    "    for i, feat1 in enumerate(fs_labels):\n",
    "        \n",
    "        if feat1 in index:\n",
    "            for j, feat2 in enumerate(fs_labels):\n",
    "                if feat2 in index:\n",
    "                    \n",
    "                    if label_group[feat1] == 1:\n",
    "                        if label_group[feat2] == 1:\n",
    "                            dist[i, j] = computeDist(df,feat1,feat2,min,max,0)\n",
    "                        elif label_group[feat2] == 2 or label_group[feat2] == 3:\n",
    "                            dist[i, j] = 1.0\n",
    "                            \n",
    "                    elif label_group[feat1] == 2:\n",
    "                        if label_group[feat2] == 1:\n",
    "                            dist[i, j] = 1.0\n",
    "                        elif label_group[feat2] == 2:\n",
    "                            dist[i, j] = computeDist(df, feat1, feat2,min,max,1)\n",
    "                            \n",
    "                    elif label_group[feat1] == 3:\n",
    "                        if label_group[feat2] == 1:\n",
    "                            dist[i, j] = 1.0\n",
    "                        elif label_group[feat2] == 3:\n",
    "                            dist[i, j] = computeDist(df, feat1, feat2,min,max,2)\n",
    "        else:\n",
    "            del_idx.append(i)\n",
    "            #print(feat1): subcort_ICV & cort_ICV : ENIGMA data에는 존재 but dictionary에는 존재x feature\n",
    "            \n",
    "    dist = np.delete(dist,del_idx,axis = 0)\n",
    "    dist = np.delete(dist,del_idx,axis = 1)\n",
    "\n",
    "    print(dist.shape)\n",
    "    print(dist)\n",
    "    print(del_idx)\n",
    "    #print(np.sum(np.isnan(dist)))\n",
    "    # X = np.delete(X,del_idx,1)\n",
    "\n",
    "    # 각 feature별로 가장 가까운 순서대로 feature의 index 저장\n",
    "    print(np.argsort(dist)[:, 1:args.nn+1] )\n",
    "\n",
    "if args.model == 'MoNet':\n",
    "    dist = np.full(X.shape, 0.5)\n",
    "    print('dist: ' + str(dist))\n",
    "    print(len(dist), len(dist[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n유전자 data decode/encode -> team3\\nocd에서는 영향 x\\nabcd, conn data와 같이 큰 data non-lin\\n        \\n<data pre-processing>\\nage에 큰 영향, age: non-linear\\n1. subcort + cor_ICV = total_ICV -> 분모 / 분자 : feature들 -> ratio 이용\\n   -> age related된 effect 보정 가능 예상\\n      thickness 는 잘 보정 x\\n      missing data 많은 col 찾기\\n   => 보정하고 다시 학습 진행\\n2. 10대가 많으면 , 나눠서 실험\\n3. 23개 feature -> dictionary.csv 파일 feature들로 새로 생성\\n   개수 적으면 RF에서 강력 -> light model이 더 잘나올듯\\n   gcn & 다른 모델 학습 진행\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "유전자 data decode/encode -> team3\n",
    "ocd에서는 영향 x\n",
    "abcd, conn data와 같이 큰 data non-lin\n",
    "        \n",
    "<data pre-processing>\n",
    "age에 큰 영향, age: non-linear\n",
    "1. subcort + cor_ICV = total_ICV -> 분모 / 분자 : feature들 -> ratio 이용\n",
    "   -> age related된 effect 보정 가능 예상\n",
    "      thickness 는 잘 보정 x\n",
    "      missing data 많은 col 찾기\n",
    "   => 보정하고 다시 학습 진행\n",
    "2. 10대가 많으면 , 나눠서 실험\n",
    "3. 23개 feature -> dictionary.csv 파일 feature들로 새로 생성\n",
    "   개수 적으면 RF에서 강력 -> light model이 더 잘나올듯\n",
    "   gcn & 다른 모델 학습 진행\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outer ID: 1\n",
      "==> Computing Pseudo Coordinates..\n",
      "Inner ID: 1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"None of [Int64Index([  0,   1,   2,   4,   6,   7,   8,   9,  10,  11,\\n            ...\\n            926, 927, 929, 930, 931, 932, 933, 935, 936, 937],\\n           dtype='int64', length=750)] are in the [columns]\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-e169163a19b6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     50\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0minner_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_id\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_id\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minner_cv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups_train_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Inner ID: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minner_id\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m                 \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_train_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m                 \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0my_train_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m                 \u001b[0mgroups_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups_val\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgroups_train_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mtrain_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups_train_val\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mval_id\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   2804\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2805\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2806\u001b[1;33m             \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2807\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2808\u001b[0m         \u001b[1;31m# take() does not accept boolean indexers\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[1;34m(self, key, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1552\u001b[0m         self._validate_read_indexer(\n\u001b[1;32m-> 1553\u001b[1;33m             \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_number\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mraise_missing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mraise_missing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1554\u001b[0m         )\n\u001b[0;32m   1555\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mkeyarr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[1;34m(self, key, indexer, axis, raise_missing)\u001b[0m\n\u001b[0;32m   1638\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmissing\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1639\u001b[0m                 \u001b[0maxis_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_axis_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1640\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1641\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1642\u001b[0m             \u001b[1;31m# We (temporarily) allow for some missing keys with .loc, except in\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"None of [Int64Index([  0,   1,   2,   4,   6,   7,   8,   9,  10,  11,\\n            ...\\n            926, 927, 929, 930, 931, 932, 933, 935, 936, 937],\\n           dtype='int64', length=750)] are in the [columns]\""
     ]
    }
   ],
   "source": [
    "# Now we import the cross-validators we want to use, depending on the specific analysis we want to perform. These are based on scikit learn's CV classes (http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection)\n",
    "\n",
    "# Since I am performing gridsearches to optimize my models, I have to use a nested cross-validation loop (inner_cv).\n",
    "# If you are not doing any nested cross-validation you can ignore the inner_cv's for now and only have to use the outer_cv lines.\n",
    "random_seed = 0 # By setting this on zero we ensure we have the exact same splits!\n",
    "\n",
    "# nested(중첩) cross-validation하기 위해 outer_cv, inner_cv 두 개 사용\n",
    "if cv_type == 0:\n",
    "    # '1. Outer CV: Site-stratified fixed fold sizes, Inner CV: Site-stratified fixed fold sizes'\n",
    "    #outer_cv = StratifiedKFoldByGroups(n_splits=10, random_state=random_seed, shuffle=True)\n",
    "    #inner_cv = StratifiedKFoldByGroups(n_splits=5, random_state=random_seed, shuffle=True)\n",
    "    outer_cv = StratifiedKFold(n_splits=10, random_state=random_seed, shuffle=True)\n",
    "    inner_cv = StratifiedKFold(n_splits=5, random_state=random_seed, shuffle=True)\n",
    "elif cv_type == 1:\n",
    "    # '2. Outer CV: Leave One Group Out, Inner CV: Group K-Fold'\n",
    "    outer_cv = LeaveOneGroupOut() # Note that here we don't have to use a random seed, as these splits will always be the same\n",
    "    inner_cv = GroupKFold(n_splits=5)\n",
    "elif cv_type == 2:\n",
    "    # '3. Outer CV: Site-stratified mixed fold sizes, Inner CV: Site-stratified fixed fold sizes'\n",
    "    outer_cv = StratifiedKFoldMixedSizes(random_state=random_seed)\n",
    "    inner_cv = StratifiedKFoldByGroups(n_splits=5, random_state=random_seed, shuffle=True)\n",
    "\n",
    "testAccArr = []\n",
    "avgValAcc = []\n",
    "avgAUC = []\n",
    "avgAUC_train = []\n",
    "sc = StandardScaler()\n",
    "\n",
    "for outer_id, (train_id) in enumerate(outer_cv.split(X, y, cov)):\n",
    "\n",
    "        print(\"Outer ID: {}\".format(outer_id + 1))\n",
    "        \n",
    "        \"\"\"Train & Test set 맞추는 코드\"\"\"\n",
    "        X_train_val = X\n",
    "        y_train_val = y\n",
    "        groups_train_val = cov\n",
    "        groups_test = cov_test\n",
    "        \n",
    "        #X_train_val, X_test = X[train_id], X[test_id]\n",
    "        #y_train_val, y_test = y[train_id], y[test_id]\n",
    "        #groups_train_val, groups_test = cov[train_id], cov[test_id]\n",
    "\n",
    "        if args.model == 'GCN' or args.model == 'DSGC' or args.model == 'ChebyNet' or args.model == 'MoNet':\n",
    "\n",
    "            del_idx=[]\n",
    "            X_graph_feat = np.delete(X_train_val,del_idx,1)\n",
    "            datAdj = computeLaplacian(X_graph_feat,args,dist)  #adjacency Matrix\n",
    "            valAcc = []\n",
    "            \n",
    "            for inner_id, (train_id, val_id) in enumerate(inner_cv.split(X_train_val, y_train_val, groups_train_val)):\n",
    "                print(\"Inner ID: {}\".format(inner_id + 1))\n",
    "                X_train, X_val = X_train_val[train_id], X_train_val[val_id]\n",
    "                y_train, y_val = y_train_val[train_id], y_train_val[val_id]\n",
    "                groups_train, groups_val = groups_train_val[train_id], groups_train_val[val_id]\n",
    "\n",
    "                X_train = sc.fit_transform(X_train)\n",
    "                X_val = sc.transform(X_val)\n",
    "                \n",
    "                #print(len(X_train), len(X_train[0]))\n",
    "                \n",
    "                #initialize weights & bias\n",
    "                # set a model\n",
    "                model = eval(args.model).Model(args, datAdj)\n",
    "                model = model.to(device)\n",
    "                \n",
    "                # set a loss function & an optimizer\n",
    "                # optimizer : sgd, adam만 가능\n",
    "                criterion = nn.CrossEntropyLoss()\n",
    "                optimizer = Optim(model.parameters(), 'sgd', lr=args.lr, weight_decay=1e-4)\n",
    "\n",
    "                #GCN\n",
    "                for epoch in range(args.epochs):\n",
    "                    train(model,epoch,X_train,y_train)\n",
    "                    val_acc = test(\"acc\", model, X_val, y_val)\n",
    "                #valAcc.append(val_acc)\n",
    "                avgValAcc.append(val_acc)\n",
    "                #print('Avg validation accuracy:{}({})'.format(np.mean(valAcc),np.std(valAcc)))\n",
    "                # if test_acc > best_acc:\n",
    "                #     # best_epoch = epoch\n",
    "                #     torch.save(model.state_dict(), 'best_'+ str(args.lr)+'.pt')\n",
    "                #     best = test_acc\n",
    "\n",
    "\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            if args.model == \"RF\":\n",
    "                params = {'randomforest__min_samples_leaf': np.arange(1, 51, 5),\n",
    "                          'randomforest__n_estimators': np.arange(10, 100, 10)}\n",
    "\n",
    "                pipe = Pipeline([\n",
    "                    ('featureExtract', VarianceThreshold()),\n",
    "                    ('scaling', StandardScaler()),\n",
    "                    ('randomforest', RandomForestClassifier(random_state=0))\n",
    "                ])\n",
    "            elif args.model == 'SVM':\n",
    "                params = {'svm__alpha': np.logspace(-4, 7, 12)}\n",
    "\n",
    "                pipe = Pipeline([\n",
    "                    ('featureExtract', VarianceThreshold()),\n",
    "                    ('scaling', StandardScaler()),\n",
    "                    (\"svm\", SGDClassifier(max_iter=1000, tol=1e-5, random_state=0))\n",
    "                ])\n",
    "            elif args.model == 'LR':\n",
    "                params = {'lr__C': np.logspace(-3, 8, 12)}\n",
    "\n",
    "                pipe = Pipeline([\n",
    "                    ('featureExtract', VarianceThreshold()),\n",
    "                    ('scaling', StandardScaler()),\n",
    "                    ('lr', linear_model.LogisticRegression(random_state=0))\n",
    "                ])\n",
    "            elif args.model == 'MLP':\n",
    "                params = {'mlp__hidden_layer_sizes': [[64,],[128,],[64,32],[128,64]]}\n",
    "\n",
    "                pipe = Pipeline([\n",
    "                    ('featureExtract', VarianceThreshold()),\n",
    "                    ('scaling', StandardScaler()),\n",
    "                    ('mlp', MLPClassifier(random_state=0,early_stopping=True))\n",
    "                ])\n",
    "\n",
    "            clf = GridSearchCV(estimator=pipe, param_grid=params, cv=inner_cv, scoring='accuracy', n_jobs=-1)\n",
    "            clf.fit(X_train_val, y_train_val,groups_train_val)\n",
    "            print(clf.best_params_)\n",
    "            \n",
    "            # AUC (Area Under Curve) -> train data로\n",
    "            y_pred = clf.predict(X_train_val)\n",
    "            avgAUC_train.append(roc_auc_score(y_train_val, y_pred))\n",
    "\n",
    "            fpr, tpr, _ = roc_curve(y_train_val, y_pred)\n",
    "            roc_auc = auc(fpr, tpr)\n",
    "            \n",
    "            fs = clf.best_estimator_.named_steps['featureExtract']\n",
    "            \n",
    "            # Accuracy\n",
    "            y_pred = clf.predict(X_test)\n",
    "            acc = accuracy_score(y_test, y_pred)\n",
    "            testAccArr.append(acc)\n",
    "            \n",
    "            # 모델의 학습 결과를 믿을 수 있는지를 보는 것이므로 \n",
    "            # train data로 AUC 계산하는게 맞는 듯\n",
    "            # AUC (Area Under Curve) -> test data로\n",
    "            #y_pred = clf.predict(X_test)\n",
    "            #avgAUC.append(roc_auc_score(y_test, y_pred))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "X_test = sc.transform(X_test)\n",
    "testAccArr.append(test(\"acc\", model, X_test,y_test))\n",
    "            \n",
    "# AUC (Area Under Curve)\n",
    "y_pred = test(\"predict\", model, X_test, y_test)\n",
    "avgAUC_train.append(roc_auc_score(y_test, y_pred.cpu().numpy()))\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred.cpu().numpy())\n",
    "roc_auc = auc(fpr, tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if args.model == 'GCN' or args.model == 'DPIEnn' or args.model == 'DSGC' or args.model == 'ChebyNet':\n",
    "print('Avg valication Accuracy:{}({})'.format(np.mean(avgValAcc),np.std(avgValAcc)))\n",
    "print('Avg Test Accuracy:{}({})'.format(np.mean(testAccArr),np.std(testAccArr)))\n",
    "print('Avg train AUC:{}({})'.format(np.mean(avgAUC_train),np.std(avgAUC_train)))\n",
    "#print('Avg AUC:{}({})'.format(np.mean(avgAUC),np.std(avgAUC)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=lw, label='ROC curve (area = %0.2f)' % roc_auc)\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
